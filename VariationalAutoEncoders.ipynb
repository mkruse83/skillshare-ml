{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "956caa30-5929-4ff2-bc6c-3906e17f6fb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# **Variational AutoEncoders (VAE)**\n",
    "This notebook covers the training of a VAE on FashionMNIST dataset.\n",
    "\n",
    "In the following sections, we shall:\n",
    "\n",
    "\n",
    "1.   Explore the FashionMNIST dataset\n",
    "2.   Implement a custom sampling layer\n",
    "3.   Implement encoder/decoder models using Functional APIs\n",
    "4.   Implement loss functions\n",
    "5.   Override Keras model train_step\n",
    "6.   Train the VAE model\n",
    "7.   Explore the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f38b31-afc8-4cd4-9a94-100ca4cf64c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First make sure we have a GPU available, or this will take a really long time to train. If you don't, refer to https://www.tensorflow.org/install/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e4cdcca-79e1-4b0b-872d-a33c89891cc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# confirm GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ded9dc63-4f23-4a46-8b26-ea1856fb9cb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's also set a consistent random seed to help make the results of this notebook more, well, consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bfa84c4-c19e-4a47-ac3e-097cb95b1f0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SEED = 123456\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # new flag present in tf 2.0+\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f65d8fe9-6b22-436c-a3e9-65e9c04a0f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploring the FashionMNIST data set ##\n",
    "\n",
    "FashionMNIST is like the handwriting recognition MNIST dataset we know and love, but with pictures of different kinds of clothes instead of pictures of people writing digits.\n",
    "\n",
    "<h2>Available Labels/Classes</h2>\n",
    "\n",
    "<table align=\"left\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th align=\"center\">Label</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td align=\"center\">0</td>\n",
    "<td>T-shirt/top</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">1</td>\n",
    "<td>Trouser</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">2</td>\n",
    "<td>Pullover</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">3</td>\n",
    "<td>Dress</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">4</td>\n",
    "<td>Coat</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">5</td>\n",
    "<td>Sandal</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">6</td>\n",
    "<td>Shirt</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">7</td>\n",
    "<td>Sneaker</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">8</td>\n",
    "<td>Bag</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td align=\"center\">9</td>\n",
    "<td>Ankle boot</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb48f1be-a0fe-4c11-b625-d63e3abbb995",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's import the data set and check its dimensions. Each image is 28x28, with 60000 training samples and 10000 test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b9f5ff-1000-483f-83ff-b00004c28984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf55a4d5-4abe-45e4-b1f7-b62c7fee10a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's always a good idea to inspect the data and get a feel for it, so let's visualize a few samples to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa91d07b-aac5-4153-a4d4-077d2584550a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "# Choose 9 samples out of 60K available in the train set\n",
    "rndSamples = np.random.choice(60000, 9)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_train[rndSamples[i]], cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b192f5-eb9b-41c9-9b9e-01a6645e2477",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We need to preprocess the data a bit before it is suitable for using with generative modeling. First, we are going to combine our training and test data together because generative models don't work via train/test measuring classification accuracy, they measure how similar generated images are to the original, using visual perception methods such as FID/IS (Frechet Inception Distance and Inception Score.) So we really just need a big pile of images.\n",
    "\n",
    "We also need to normalize the data; the raw pixel data is in the range 0-255, but sigmoid activation values are 0-1, so we'll scale all the pixels to 0-1 by just dividing them by 255.\n",
    "\n",
    "And, we need to add an extra dimension since convolutional layers expect 3 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b35f41-3094-48ac-9013-2b5de6ae2278",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = np.concatenate([x_train, x_test], axis=0)\n",
    "# Please note expand_dims converts images from 28x28 to 28x28x1\n",
    "# Since convolutional layers expect 3 channels\n",
    "dataset = np.expand_dims(dataset, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5467d27a-ce7c-483a-a6a1-ec872d468f63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Custom Sampling Layer ##\n",
    "\n",
    "Our encoder is trying to learn a probability distribution z given data X. The distribution is defined by Mu and Sigma.\n",
    "\n",
    "Then, the decoder samples randomly from the z distribution.\n",
    "\n",
    "The problem is that the random sampling is not differentiable, which is needed for backpropagation training. But by introducing a parameter Epsilon we make it a deterministic, differentiable operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd6650a-53da-43c8-8681-2abc6e7aba5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Create a sampling layer\n",
    "class SamplingLayer(layers.Layer):\n",
    "  \"\"\"Reparameterization Trick z = mu + sigma * epsilon\"\"\"\n",
    "\n",
    "  def call(self, inputs):\n",
    "    zMean, zLogVar = inputs\n",
    "    batch = tf.shape(zMean)[0]\n",
    "    dim = tf.shape(zMean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return zMean + tf.exp(0.5 * zLogVar) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c911b04-a65b-4959-9951-599c174360a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Encoder / Decoder ##\n",
    "\n",
    "Our encoder uses Keras functional API's to create a non-sequential model, as we need to output two values in parallel (mean and variance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159e061f-0f62-4198-afa6-259381975ad0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def buildEncoder(latentDim, encoderInputs):\n",
    "\n",
    "  # Given a batch of images the convolutional block extracts the features\n",
    "  l1 = keras.models.Sequential([\n",
    "    layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation=\"relu\")\n",
    "  ])\n",
    "\n",
    "  # pass the inputs through the convolutional block\n",
    "  x = l1(encoderInputs)\n",
    "\n",
    "  # a dedicated layer to learn mean in parallel\n",
    "  zMean = layers.Dense(latentDim, name=\"z_mean\")(x)\n",
    "\n",
    "  # a dedicated layer to learn variance in parallel\n",
    "  zLogVar = layers.Dense(latentDim, name=\"z_log_var\")(x)\n",
    "\n",
    "  # now the reparametrization trick to find z as defined by mean and variance\n",
    "  z = SamplingLayer()([zMean, zLogVar])\n",
    "\n",
    "  # the actual model which takes the images as input and returns mean, variance, and distribution\n",
    "  # please note the zMean and zLogVar are not the final output of the encoder, but\n",
    "  # they are used in the Kullback-Leibler Divergence Loss (explained below)\n",
    "  return keras.Model(encoderInputs, [zMean, zLogVar, z], name=\"encoder\")\n",
    "\n",
    "\n",
    "# trigger the function to actually build the model\n",
    "encoderInputs = keras.Input(shape=(28, 28, 1))\n",
    "encoder = buildEncoder(2, encoderInputs)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f439fcd-6eb3-44ca-8e70-e461c265846f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And now we'll implement the decoder in a similar fashion. Note the use of Conv2DTranspose instead of Conv2D; we use the transpose convolution to generate images going from lower resolutions to higher ones. That's the real heart of this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b051fa0b-ea7d-4fe5-b8f5-0b8b532e1c13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def buildDecoder(latentInputs):\n",
    "\n",
    "  l1 = keras.models.Sequential([\n",
    "    layers.Dense(7*7*64, activation=\"relu\", input_shape=(latentInputs.shape[1],)),\n",
    "    layers.Reshape((7,7,64)),\n",
    "    layers.Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
    "    layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
    "  ])\n",
    "\n",
    "  return keras.Model(latentInputs, l1(latentInputs), name=\"decoder\")\n",
    "\n",
    "# build the actual model\n",
    "latentInputs = keras.Input(shape=(2,))\n",
    "decoder = buildDecoder(latentInputs)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d80027e4-9c36-4559-9109-e323821fc135",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loss Functions ##\n",
    "\n",
    "VAE has TWO loss functions! First is the reconstruction loss, which penalizes images that are not similar to the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e0add18-8840-41ae-b031-ca6d76f709dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reconstructionLoss(data, reconstructed):\n",
    "  return tf.reduce_mean(\n",
    "      tf.reduce_sum(\n",
    "          keras.losses.binary_crossentropy(data, reconstructed),\n",
    "          axis=(1, 2)\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fc60b80-c8a2-449f-b794-1424976c04cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we have the Kullback-Liebler Divergence loss function, also called Earth Mover loss from the analogy of figuring out how much dirt needs to be moved to transform the shape of the UK to the shape of the US.\n",
    "\n",
    "What it really does is measure the distance between two probability distributions. It penalizes the model if it learns a probability distribution \"z\" that is different from the original probability distribution of the data \"X\".\n",
    "\n",
    "Mathematically it is the same thing as cross-entropy - entropy, which means it works well for backpropagation and log-likelihood minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f5a352-7347-425d-975d-bdfee19cba5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def klDivergenceLoss(zMean, zLogVar):\n",
    "  return tf.reduce_mean(\n",
    "      tf.reduce_sum(\n",
    "          -0.5 * (1 + zLogVar - tf.square(zMean) - tf.exp(zLogVar)),\n",
    "          axis=1\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f777fc-a998-4d36-8f34-671a9b6280b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "So now we'll combine these into the total loss function, which just weights them and sums them up. Think of that weight (alpha) as another hyperparameter you can tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffe99958-73a0-4daf-b393-a255a90f454e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calcTotalLoss(data, reconstructed, zMean, zLogVar):\n",
    "  loss1 = reconstructionLoss(data, reconstructed)\n",
    "  loss2 = klDivergenceLoss(zMean, zLogVar)\n",
    "  klWeight = 3.0\n",
    "  return  loss1, loss2, loss1 + klWeight * loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f4a8e5-d88e-484c-8cbd-92e7e8a98a9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overriding train_step ##\n",
    "\n",
    "Since we are using a custom loss function on mu and sigma, we need to override how the loss is calculated. We need to compare X_original and X_reconstructed at the same time while calculating the Kullback-Liebler (KL) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4c5afd-25d4-4c7a-ace9-f3932586eeb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "  def __init__(self, encoder, decoder, **kwargs):\n",
    "    super(VAE, self).__init__(**kwargs)\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    # register total loss as an observable metric in the model training history\n",
    "    self.totalLossTracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "    self.ceLossTracker = keras.metrics.Mean(name=\"ce_loss\")\n",
    "    self.klLossTracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "  # This are all observable metrics \n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [\n",
    "        self.totalLossTracker,\n",
    "        self.ceLossTracker,\n",
    "        self.klLossTracker\n",
    "    ]\n",
    "\n",
    "  # Now calculate loss + calculate gradients + update weights\n",
    "  def train_step(self, data):\n",
    "    # Gradient tape is a recording of all gradients for the trainable \n",
    "    # weights that need to be updated\n",
    "    with tf.GradientTape() as tape:\n",
    "        # forward path\n",
    "        zMean, zLogVar, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        ceLoss, klLoss, totalLoss = calcTotalLoss(data, reconstruction, zMean, zLogVar)\n",
    "    # backward path\n",
    "    grads = tape.gradient(totalLoss, self.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "    # keep track of loss\n",
    "    self.totalLossTracker.update_state(totalLoss)\n",
    "    self.ceLossTracker.update_state(ceLoss)\n",
    "    self.klLossTracker.update_state(klLoss)\n",
    "\n",
    "    # return the loss for history object\n",
    "    return {\n",
    "        \"total_loss\": self.totalLossTracker.result(),\n",
    "        \"ce_loss\": self.ceLossTracker.result(),\n",
    "        \"kl_loss\": self.klLossTracker.result()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ee090a-a337-48fd-a810-68e2851ff580",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train the VAE! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9557fa5b-30dd-4350-9bed-e1d2111147f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "history = vae.fit(dataset, epochs=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a2fc540-ec66-4e4d-95ab-5e11b46777be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's visualize how the loss functions changed over each epoch; this can inform at what point it makes sense to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c67e35b-a03f-4e10-bf9e-1d1edb3e65b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 9))\n",
    "plt.plot(history.history.get('total_loss'), label=\"total loss\")\n",
    "plt.plot(history.history.get('ce_loss'), label=\"reconstruction loss\")\n",
    "plt.plot(history.history.get('kl_loss'), label=\"KL loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2c57240-53f7-4b7a-87c4-a968deb490ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's zoom into the KL loss so we can see it better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b9bb8f0-789f-49e8-b469-5869a71c12f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 9))\n",
    "plt.plot(history.history.get('kl_loss'), label=\"KL loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9644f8ee-f382-47eb-bf74-eb020c301c23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explore the results ##\n",
    "\n",
    "Let's use a mean (mu) of 1 and variance (sigma) of 2 (just a guess to start with). Play around with this, try different values to get different types of objects (shirts, bags, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "226318a7-0197-4004-a1ef-e2afc50132eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "synth = vae.decoder.predict([[1, 2]])\n",
    "plt.axis('off')\n",
    "plt.imshow(synth.reshape((28,28)), cmap=\"Greys_r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4b8918-f312-49bf-b93b-25178989bec6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's generate 256 images entirely at random. All we have to do is guess the Z distribution (defined by mu, sigma). Don't worry, we will get to a systematic method to determine the mu and sigma for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a179921-4edc-486e-9035-cd221d7a9c02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "z = np.random.normal(loc=0, scale=4, size=(256,2))\n",
    "synth = vae.decoder.predict(z)\n",
    "\n",
    "plt.figure(figsize=(28,28))\n",
    "\n",
    "for i in range(256):\n",
    "    plt.subplot(16,16,i+1)\n",
    "    plt.imshow(synth[i].reshape((28,28)), cmap=\"Greys_r\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939464ea-934e-4fc2-9309-58c9ff6c0db4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As promised, we can find the mapping of mu and sigma for each category. Just map an image from each category through the encoder and observe the output. Then use those outputs to conditionally instruct the decoder to sample from a particular category. This isn't a guaranteed solution though - for concrete results look up \"Conditional Variational AutoEncoders\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebacd422-3245-4fc0-af71-e607cd854b97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idx = 1280\n",
    "batch = np.expand_dims(x_train[idx], axis=0)\n",
    "batchOfImages = np.expand_dims(batch, axis=-1).astype(\"float32\") / 255\n",
    "print(batchOfImages.shape)\n",
    "# obtain z(mu,sigma) for the given image\n",
    "_, _, z = vae.encoder.predict(batchOfImages)\n",
    "\n",
    "# now reconstruct a similar image\n",
    "synth = vae.decoder.predict([z])\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96dd576e-586b-4503-81a3-5a4d2f74ee9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the reconstructed image could be different from the original, depending on how long the model has been trained and whether it has reached a global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "603176a8-9a28-4c59-9543-948b9efc35b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 28))\n",
    "\n",
    "# original image\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(x_train[idx], cmap=\"Greys_r\")\n",
    "\n",
    "\n",
    "# reconstructed\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off')\n",
    "plt.imshow(synth[0].reshape((28,28)), cmap=\"Greys_r\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4efb81b9-cf52-47b6-a09a-41d700cdaae5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAE can be used in unsupervised learning of large text or unlabeled images corpus\n",
    "to cluster the samples into categories\n",
    "\"\"\"\n",
    "labels = np.concatenate([y_train, y_test], axis=0)\n",
    "meu, _, _ = vae.encoder.predict(dataset)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(meu[:, 0], meu[:, 1], c=labels)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"meu[0]\")\n",
    "plt.ylabel(\"meu[1]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5682808f-1c04-4d76-a8c8-a9e35fb4da80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "VariationalAutoEncoders",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
